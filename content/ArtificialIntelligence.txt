# Some information about Artificial Intelligence

What is AI?

  An artificial intelligence is a computer system that has been trained on data to make decisions.

AI uses patterns

  AIs make predictions based on patterns.

  If there is no pattern in the data given to the AI, the AI cannot reliably make precise, accurate predictions.

  For example, if we flip a fair, random coin N times, we will get a sequence of N results. Then the result of the next flip has a 1/2 chance of being heads and a 1/2 chance of being tails. If we ask an AI to use that information to predict the next result, then whether the AI predicts heads or tails, that result will have a 1/2 chance of being incorrect.

 - What is AI?

We want AI models to be generalizable

  When we train an AI model to give the correct answers for some training examples, it can theoretically just learn the exact answers to the specific examples.

  This would allow the computer to make accurate predictions during training, but probably wouldn't be very useful after that.

  In the cases where someone wants a tool that memorizes the answers, they don't usually train an AI for that task.

 - What is AI?

More complicated models benefit from more data

  If our model is complicated and our dataset is small, we run the risk of overfitting.

  If we know the complexity of a model and the amount of data that the model was trained on, we can use [Hoeffding's inequality](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality) to compute the probability of the model having a high error rate.

 - What is overfitting?

 - We want AI models to be generalizable

Decision trees

  A decision tree is a type of AI model. When a tree makes a prediction about a specific datapoint, it evaluates a series of yes/no questions until it arrives an answer.

 - What is AI?

Neural networks

  A neural network is a type of AI model.

  It involves a collection of nodes, each of which is connected to many other nodes.

  Most nodes combine their inputs and apply a nonlinear function to produce an output.

  The details of how a node combines its inputs is dependent on its weights.

  The weights of node connections in a graph make up the parameters of the neural network.

  Neural networks can model many different functions and can be trained.

 - What is AI?

What is a deep neural network?

  A deep neural network is a neural network with at least 3 layers

 - Neural networks

What is deep learning?

  Deep learning refers to training a deep neural network

 - What is a deep neural network?
 - Training neural networks

Training neural networks

  A neural network is a complicated function that makes predictions for an input based on its internal parameters.

  Training a neural network involves changing its parameters in such a way that its predictions for a training set get more accurate.

  Usually the way this works is:

  1. The user identifies some training data

  2. The user chooses a loss function

  3. Backpropagation identifies the gradient of the loss on this training set with respect to the network parameters

  4. Gradient descent repeatedly uses backpropagation to compute the gradient, and uses the gradient to change the network's parameters to reduce the loss

  The purpose is not simply to minimize the loss on training sets, but to find parameters that can accuractly predict previously unseen data (testing sets) - generalization.

 - Neural networks

 - What is a loss function?

 - Gradient descent

 - What is backpropagation?

Gradient descent

  [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) is an algorithm that attempts to find the minimum of an arbitrary, unknown function.

  The way that it works is:

  1. Choose a point to start at.

  2. Compute the gradient of the function at that point.

  3. Move a certain distance in the direction of the gradient.

  4. Repeat until satisfied.

  Note that this isn't guaranteed to find the absolute minimum of a function.

  If the function has multiple regions separated by a mountain and the initial guessed point was on one side of the mountain, then other points on the other side of the mountain may not be considered and may have even smaller values.

  One approach to increase the probability of finding the absolute minimum is to run this algorithm several times, each with a different initial guess.

 - What is a gradient?

Learning rate

  Learning rate is the rate that a Gradient Descent algorithm updates its parameters (w and b) - by learning rate * gradient.

 - Gradient descent

Large Language Models

  Large language models are neural networks that attempt to understand language.

  They're often trained to predict a specific word in a text given the rest of the surrounding words.

 - Neural networks

What is a loss function?

  A loss function is a function that compares the output of an AI model (for a given input) to the correct answer and computes a penalty.

  This penalty is called "loss".

  Smaller values are better, with 0 being the best

  For example, suppose we have an AI that is supposed to identify objects.

  * If we show it an apple and it outputs the text "apple", we might assign that a loss of 0.

  * If we show it an apple and it says "cherry", we might assign that a loss of 0.1.

  * If we show it an apple and it says "chair". we might assign that a loss of 1.

 - What is AI?

What is backpropagation?

  Backpropagation is a way of determining the gradient of the output of a neural network with respect to each of the network's parameters

  Backpropagation starts at the last layer in the neural network and computes the gradient of each output of the network with respect to each parameter in the last layer of the network.

  Then, backpropagation proceeds to the previous layer (this is why it's called "backpropagation") and computes the gradient of each output of the network as a function of each parameter in that layer of the network.

  This process continues until each gradient is computed.

 - What is a gradient?

 - Neural networks

What is a gradient?

  A gradient is a list of slopes, with one slope per dimension.

  A gradient tells how much one value changes as a result of a small change in another value.

  For example, for a function `z = 3x + y*y`, the gradient is `(3, 2y)` because increasing `x` by a tiny amount will increase `z` by three times that amount, and increasing `y` by a tiny amount will increase `z` by that amount times `2y`.

Expectation Maximization

  [Expectation maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) is a method for guessing appropriate values for the parameters of a model based on some datapoints, most or all of which are usually unlabelled.

  It involves these steps:

  1. Choose some initial guesses for the parameters

  1. Repeatedly do this:

      * For each datapoint and each label, compute the probability that that datapoint has that label based on the model parameters.

      * Compute the most likely model parameters based on the previously computed labels.

  An expectation maximization algorithm is a semi-supervised learning algorithm

 - What is semi-supervised learning?

What is supervised learning?

  Supervised learning is learning where each datapoint in the training data knows what its classification is

What is semi-supervised learning?

  Semi-supervised learning is learning where some answers may be known and some may be unknown

  If all answers are known, it is the same as supervised learning

  If no answers are known, it is the same as unsupervised learning

 - What is supervised learning?

 - What is unsupervised learning?

What is unsupervised learning?

  Unsupervised learning is learning where no answers are known.

  A common type of unsupervised learning is to cluster datapoints into groups

What are embeddings for?

  If a computer has two objects and wants to know how similar they are, it often works well to compute an embedding for each of the objects and compare the embeddings.

  If there are lots of objects to compare, this only requires computing each embedding once.

  If you want to search for similar objects, it's easier to search for similar embeddings because they are numbers that can be used as keys in a data structure.

 - What is an embedding?

What is an embedding?

  An embedding is a list of numbers that represents an object

  For example, if you want to compute an embedding for a fruit, you could choose to make a list of these numbers:

  * The average amount of red in the fruit

  * The average amount of green color in the fruit

  * The average amount of blue color in the fruit

  * The length of the fruit

  * The mass of the fruit

 - What is a vector?

What is a vector?

  A vector is a list of numbers

What is a GPU?

  A graphics processing unit is a computer system made of a large number of simple processors. Each processor must execute the same instruction at a time but operates on different data.

Why is a GPU called a GPU?

  A GPU is useful for determining what information to display on a screen. That was what they were first used for.

  Nowadays a GPU may also be used for artificial intelligence tasks that also involve running similar instructions on large amounts of data.

 - What is a GPU?

What is pytorch?

  Pytorch https://github.com/pytorch/pytorch is a python-based platform to train machine learning models with CPU/GPU.

  You can install pytorch by `pip3 install torch` and import pytorch in python by `import torch`.

 - What is a GPU?

 - Neural networks

What is a tensor?

  Tensor is a form to hold n-dimensional arrays in a GPU.

  You can convert Numpy arrays to tensors by `your_tensor = torch.from_numpy(your_array.np())`.

  You can save tensors to GPU by `your_tensor.to('cuda')`.

 - What is a GPU?

 - What is pytorch?

 - What is Numpy?

 - What is Python?

What is Numpy?

  Numpy is a python package to do computation on arrays.

  You can install Numpy by `pip install numpy` and run Numpy in python by `import numpy as np`.

 - What is Python?

How can I import Pandas data as tensors?

  Pandas is a python package that works with datasets.

  You can install Numpy by `pip install pandas` and run Numpy in python by `import pandas as pd`.

  You can summarize your data in CSV files and import your data using Pandas by `data = pd.read_csv(data_file)`.

  Then you can convert the non-numeric columns of your data as categories of 0s and 1s by `non_num_data = data.loc[:,['non_num_col1','non_num_col2',...]]` and `non_num_data = pd.get_dummies(non_num_data, dummy_na=True)`.

  You can save the numeric columns of your data as `num_data = data.loc[:,['num_col1','num_col2',...]]`.

  You can check your data by `num_data.head()` and `non_num_data.head()`, and check their size (number of rows and number of columns) by `num_data.shape` and `non_num_data.shape`.

  Now you can convert your data into tensors by `non_num_data = torch.tensor(non_num_data.to_numpy(dtype=float))` and `num_data = torch.tensor(num_data.to_numpy(dtype=float))`.

 - What is a tensor?

 - What is Python?

How to deal with missing values in training/testing data?

  Dealing with missing data (Na or NaN or '') properly is very important in data science.

  An example of how missing data (as 0s) fooling people: [Major data analysis errors invalidate cancer microbiome findings](https://journals.asm.org/doi/10.1128/mbio.01607-23)

  1) You can delete all rows and columns that contain missing data.

  2) You can convert missing data into two columns: column 1 shows all the existing categories; and column 2 shows whether a category is missing by `data_with_categories = pd.get_dummies(input_with_missing_data, dummy_na=True)`.

  3) You can replace missing data with averages by `data_with_means = input_with_missing_data.fillna(input_with_missing_data.mean())`.

  Most importantly, you should always check whether the pattern you get from a model is associated with missing data.

 - What is Python?

 - How can I import Pandas data as tensors?

Sampling size VS uncertainty

  When a mathematical model has more data, it can often make more precise predictions.

  * For example, if the model is that a certain value comes from a Gaussian distribution, then multiplying the sample size by (n) should divide the uncertainty by `n^0.5`

      * By going from 10 to 1000 observations, we see a 10-fold reduction of uncertainty.

      * The next 1000 observations only offer 1.41 times reduction.

 - Probability VS statistics

What is a training dataset?

  A training dataset is a collection of data used to train a machine learning model.

What is a testing dataset?

  A testing dataset is a collection of data used to evaluate the performance of a trained machine learning model during training. It contains data that the model has not seen during training to evaluate how well the model generalizes to unseen data.

 - What is a training dataset?

What is a validation dataset?

  A validation dataset is a collection of data used to evaluate the performance of a final machine learning model. The model should not have seen this data (or similar data) during training and testing!

 - What is a training dataset?

What is a label in machine learning?

  A label is a classification of a datapoint.

  If a label is known, we can use it to help train a machine learning model.

  If a label is not known, we can try to predict it using a machine learning model.

 - What is a training dataset?

What is a feature in machine learning?

  A feature is a variable that a prediction can be based on.

  For example, if you have a piece of fruit, you can measure its height; height is a feature.

 - What is a training dataset?

 - What is a label in machine learning?

What is a bias term in machine learning?

  Bias is a parameter that allows the model to shift the output independently of the input. The bias determines the value of the estimate when all features are zero. It acts like an offset or intercept in linear equations, `y= w1`(weight 1)`*x1`(feature 1)` + w2*x2 + ... + wn*xn + b` (bias).

 - What is optimization?

What is optimization?

  Optimization is the process of modifying something to improve its performance.

What are hyperparameters?

  Hyperparameters are parameters that affect the process of training an AI model but are not directly recorded within the model itself.

  For example, when using Minibatch SGD, two hyperparameters are the learning rate and batch size.

  Hyperparameters can be tuned automatically by Bayesian optimization.

 - Learning rate

 - Minibatch SGD

What is inference?

  Inference can mean:

  * Making predictions using a mathematical model

      * We try to use the word "prediction" here because it is clearer.

  * Drawing conclusions based on evidence

Stochastic gradient descent (SGD)

  Stochastic gradient descent is an algorithm for optimizing the parameters of a machine learning model.

  It estimates the gradient of the loss function using only one training example at a time.

  This is generally much faster than Gradient Descent.

 - Gradient descent

Minibatch SGD

  Minibatch SGD is an algorithm for optimizing the parameters of a machine learning model.

  It estimates the gradient of the loss function using a subset of the training examples at any one time.

  The size of a minibatch is usually between 32-256, a multiple of a large power of 2.

  Steps:

  0) Initialize the model parameters by random values

  1) Randomly sample a minibatch.

  2) Compute the gradient of the average loss for each model parameter (w and b).

  3) Multiple the gradient by the learning rate and subtract the result from the current parameter value. Parameter_new = Parameter - learning rate * gradient (parameter)

  Note that random sampling makes this process not deterministic.

 - Stochastic gradient descent (SGD)

How to train a neural network in Pytorch?

  1) Define a neural network (model) using `def forward()`. `forward()` defines the calculation on inputs (x) to generate outputs (y).

  2) Choose an optimization method (optimizer) using `torch.optim`, which improves the parameters in the neural network.

      * Example: Stochastic gradient descent (`torch.optim.SGD()`).

      * Example: Adam optimizer (`torch.optim.Adam()`).

  3) Choose a loss function (loss_fn), that measures how wrong the predictions (y_predicted by the model) are compared to the truth (y).

      * Example: MAE (mean absolute error) loss function for regression problems (predicting a number)

      * Example: Binary cross entropy loss function for classification problems (predicting one thing or another).

  4) Prepare training data (60-80%), testing data (10-20%), and validation data (10-20%).

  5) Define the training loop:

    Set model mode to training by `model.train()`

    Pass the training data through the model by `y_predicted = model(x_train)`

    Calculate loss by `loss = loss_fn(y_predicted,y_train)`

    Clean up old gradients in the optimizer by `optimizer.zero_grad()`

    Backpropagation to compute the gradient of the loss for each model parameter by `loss.backward()`

    Optimize the model parameters by `optimizer.step()`

    NOTE: loss should be calculated before backpropagation; gradients should be cleaned before backpropagation; parameters should be optimized after backpropagation

  6) Define the testing loop:

    Set model mode to evaluation (model.eval())

    Pass the testing data through the model by `y_predicted = model(x_test)`

    Calculate loss by `loss = loss_fn(y_predicted,y_test)`

    Evaluate model performance, for example, visualize average loss or accuracy for every 10 epochs

    NOTE: testing loop doesn't contain backpropagation and paramater optimization.

 - What is pytorch?

 - Neural networks

 - What is a validation dataset?

How can I train a neural network on a GPU?

  1) Check whether a GPU is available by `torch.cuda.is_available()` and set `device = "cuda" if torch.cuda.is_available() else "cpu"`

  2) Move your neural network to GPU by `model_1.to(device)`

  3) Move all data to GPU by

  `x_train = x_train.to(device)`

  `x_test = x_test.to(device)`

  `y_train = y_train.to(device)`

  `y_test = y_test.to(device)`

 - How to train a neural network in Pytorch?

 - What is a GPU?

 - What is pytorch?

 - Neural networks

What is overfitting?

  Overfitting refers to the process of training a model with lots of parameters on a small dataset in a way that causes the model to essentially just memorize the training set rather than find any patterns.

  Even if an AI model is not directly memorizing the training set, it is possible that it is essentially memorizing the training set via one of:

  * Examining small, irrelevant changes in one or more attributes

  * Computing a new attribute by combining other attributes, and then memorizing the correct answer for each of value of this new attribute

      * This is how a hash table works

          * Note that a hash table is not intended to predict results for other values. With a hash table, memorizing the data is intended!

 - What is a hash table?

Handling overfitting in neural networks

  Things we can do to prevent overfitting in a neural network

    1) Design representative training data and testing data. For examples, bacterial species in testing data should not be included in training data.

    2) Early stop: stopping the training when testing loss reaches a minimum (it could be a local minimum though).

    3) Drop out: randomly dropping out X% nodes (set activations to 0) in each epoch of training. It forces the neural network to not rely on any single node.

    4) I guess trying different neural network size, structure, optimizers, activation functions?

 - What is overfitting?

 - Neural networks

Handling neural network local minimums

  Neural networks are often trained via gradient descent, which does not guarantee that the training process will find the set of parameters that produce the absolute minimum loss for the given training set. Sometimes it will find a local minimum instead.

  Things we can try to be not stuck with local minimum

    1) adapt learning rates to the landscape of gradient - try different optimizers

    2) try different initial states (random values we give to parameters before trainig)

 - Gradient descent

 - Neural networks

Common problems training a neural network

 - Handling overfitting in neural networks

 - Handling neural network local minimums

What is a recurrent neural network (RNN)?

  A recurrent neural network is a network that remembers some past inputs for previous steps in a task.

  This means its output at any given step is dependent on both the input (x_t) for the current step but also some special outputs from the previous step (h_t-1).

  An RNN can work well with sequential data, such as voice, sentences, DNA/protein sequences, ..., where current states are determined by both input and previous states.

  The loss of RNN is the sum of the losses over all time steps (y_predicted_t VS y_t).

 - Neural networks

 - What is a loss function?

Problems in training an RNN

  Backpropagation in RNN changes the parameters way more (t) times than a simple neural network - a parameter can go too big (explode) or too small (vanish) very quickly.

  Things we can do:

    1) Better activation functions - ReLU

    2) Initialize the parameters smartly - initialize weights to identity matrix, initialize biases to zero. This helps to prevent the weights from shrinking to zero. Why?

    3) Gated call - add more computations within the recurrent cell to selectively keep or remove information, such as long short term memory (LSTMs).

  It's hard for an RNN to deal with long sequencial data efficiently. It's hard to paralize RNN computation.

  Solution:

    Not think about each time step in isolation. Let's learn which part of the sequence data is actually important - attention!

 - What is a recurrent neural network (RNN)?

What is an attention head?

  An attention head (also described [here](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021#8607)) takes as input a matrix representing a list of embeddings of tokens, and produces as output a list of attention values.

  An attention head has three matrices of parameters that can be learned like other parameters in a neural network. Let's call these matrices P1, P2, and P3.

  When evaluating an attention head for a specific input matrix, we do this:

  ```

  Queries = Input * P1

  Keys = Input * P2

  Values = Input * P3

  ```

  Then, for each token, we compute the similarity of its entry in Queries with each other entry in Keys via a dot product. This generates a list of similarities.

  Then, we renormalize these similarities so that the total similarity for a given token equals 1.

  Then, for each token, we multiply each similarity by the corresponding entry from the Values.

  The output of this process is the attention values.

  Each attention head in a model is given the raw input directly (usually tokenized words), but has its own set of parameters that can be learned independently.

 - What is an embedding?

 - Neural networks

 - Training neural networks

What is multi-head attention?

  Multi-head attention is the idea of using and training several attention heads separately, and concatenating their results into a single output.

 - What is an attention head?

What is a transformer?

  A [transformer](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)) is a machine learning model that consists of several layers.

  Each layer contains a multi-headed attention block, and a neural network.

  The attention blocks help the model to identify important words.

  The neural network allows the model to learn more complicated patterns, and generates the output for that layer.

  The output of the last layer of the transformer is considered to be the output of the transformer.

 - What is multi-head attention?

 - Neural networks

Why use a transformer?

  A transformer can be more easily parallelized than some other algorithms like CNN or RNN.

  A transformer isn't affected by the vanishing-gradient problem that can affect RNNs.

 - What is a transformer?

 - What is a recurrent neural network (RNN)?

 - Problems in training an RNN

 - What is a convolutional network (CNN)?

What is a convolutional network (CNN)?

  CNN applies convolution functions to the input (usually an image).

  The convolution function is called a filter. CNN applies a filter to a small patch of the input (image) to pick up patterns from a small region.

  The filter (weights) is trained by back propagation.

  A CNN usually has multiple filters to pick up different patterns.

  The output of a filter applied to a patch of input is called feature. A feature map contains all output of a filter applied to the whole input.

  Activation, such as ReLu, is also necessary in CNN to learn non-linear relationships.

  Usually, we want to decrease data size and increase filters to learn more patterns with simialr number of parameters.

  This is done by pooling, which pool features of nearby image positions into one number, e.g. max or average.

 - Neural networks

 - What is a feature in machine learning?

What is generative modeling?

  Generative modeling takes as input training samples from some distribution and learn a model that represents that distribution.

  Generative modeling can be used for generating new data, uncovering underlying features (latent variables) in a dataset, and detecting outliers.

  We can use outliers during training to improve even more!

 - Neural networks

Autoencoders

  Learning a lower-dimensional feature of an input in an unsupervised (unlabelled) way.

  We want to compress the data to a lower-dimensional feature, to learn the latent variables/space (underlying or truly explanatory features) of the data.

  How to do that? We can see whether we can reconstruct the original data using the latent space, e.g. loss = mean squared error (original data, reconstructed data)

  Autoencoding is a type of compression. Dimensionality of latent space impacts the reconstruction quality.

 - What is generative modeling?

Variation Autoencoders

  Variation autoencoders (VAEs) make autoencoders a generative model by adding randomness (based on some probability distribution) to the latent space.

  For each variable in the latent space, we compute mean and standard deviation to represent a probability (normal) distribution.

  Then we sample from those probability distributions to generate new data.

 - What is generative modeling?

 - Autoencoders

Regularization on the latent space

  We choose an assumption or prior on the latent distribution to regulatarize the training.

  For example, we use normal Gaussian distribution as a prior to

  1) encourage encodings to distribute evenly around the center of the latent space;

  2) penalize models that cheat by memorizing the data - which would not likely to follow a normal distribution

  We want the latent space to be

  1) continuity: points that are close in latent space should have similar content after decoding

  2) completeness: samples from latent space should have meaningful content after deconding

 - Variation Autoencoders

 - What is generative modeling?

 - Autoencoders

How to interpret latent space?

  We can hold all vairables but one fixed and see how changing that single variable impact the reconstructed output.

 - Variation Autoencoders

 - What is generative modeling?

 - Autoencoders

Disentanglement in machine learning

  We want the latent variables to be uncorrelated with each other -> to maximize the amount of information the model learns across a few set of latent variables.

  This can be done by disentanglement - to enforce diagonal prior on the latent variables to encourage independence.

  For example, to tune the relative strength of reconstruction term and regularization term (by increasing it) in the loss calculation.

 - Variation Autoencoders

 - What is generative modeling?

 - Autoencoders

 - Regularization on the latent space

What is a generative adversarial network (GAN)?

  Sometimes, when we focus on making the latent variables more interpretable, we could sacrifice the ability of the model to generate new samples.

  We can use GANs to just generate good new samples without trying to decode and interpret latent variables.

  GANs don't model density, but sample from something simple (e.g. Gaussian noise), learn a model that transform the distribution of noise to the real data distribution -> to generate new samples that fall in the real data distribution.

  GANs have a generator and a discriminator in competition with each other (adversaries).

  The generator turns noise into a real data distribution to try to trick the discriminator.

  The discriminator tries to identify real data from fakes created by the generator.

  The generator and discriminator are trained and optimized together -> the goal is to make the generator reproduces the true data distribution.

  After training, we use the generator to create new data that's never been seen before.

 - What is generative modeling?

Conditional GANs

  We supply random noise + conditioning factor as the starting point for the generator to guide the generation.

  For example, paired translation - we can use pairs of inputs, e.g. a real scene and a segmentation map. The trained GANs can generate, e.g. a real scene based on a segmentation map.

  We can also input two related data distribution (no explicit pairs) to learn relationships/transformation that links these two domains - cycleGAN (domain transformation).

 - What is generative modeling?

 - What is a generative adversarial network (GAN)?

What is reinforcement learning?

  Reinforcement learning takes state-action pairs as input and tries to maximize future rewards over many time steps.

  Terms

  Agent: Something that can take actions - here we refer to AI models that can take actions

  Environment: the world in which the agent exists and operates

  Observation: how the environment interacts back to the agent

  State: an immediate observation that the agent is present in

  Action: a move that an agent makes in the environment

  Reward: feedback that measures the success/failure of the agent's action, can be immediate or delayed

  Discounting factor: a discounting factor in the environment provides that dampens the effects of a reward over time - future reward may be worth much less than immediate reward.

  Total reward: the discounted sum of all rewards obtained from time t to the future.

  Q function: computes the expected total reward (from t to the future) an agent in state (s) can receive by executing a certain action (a).

  Policy function (pie): to infer the best action (which maximizes the future reward) to take at its state (s). We can evaluate pie function using the Q function - to find an action a that gives the highest Q value.

 - Neural networks

Value learning in reinforcement learning

  Is to find Q function (s,a).

  For each state, we can take all possible actions, feed it into the Q function, compute the Q values, and find the best action as the one gives the maximum Q value.

  Or for each state, the Q function outputs Q value for each action - more efficient. Then we infer the optimal policy pie function from the action yielding the max Q value.

  Limitation: cannot learn continuous action spaces.

 - What is reinforcement learning?

Policy learning in reinforcement learning

  Is to find policy function pie(s). More direct way to find the best action than value learning.

  The policy learning finds a policy function that outputs the probability of an action being the best action given a state.

  Then we can ask the model to learn parameters (e.g. mean and variance) of the probability distribution.

  We can then sample from the probability distribution, and the model can even pick an action that doesn't yield the best reward, to explore more.

  loss = -logProbability(action_t|state_t) * Reward_t

 - What is reinforcement learning?

 - Value learning in reinforcement learning

How to build AlphaGo?

  Train a supervised model with human data of play

  Self-play of pretrained models against each other with reinforcement learning

    Increase the probability of all the actions of the agent that won.

    Decrease the probability of all the actions of the agent that lost the game.

  Run millions or billions of steps

  Intuition about the board state -> achieve state in and value out (Q function)

  You can skil step 1 (observing human data) to achieve even better performance while it would take longer training steps.

  Summary: human expert positions -> classification -> supervised learning policy network -> self-play - reinforcement learning policy network - self-play -> self-play data -> regression -> value network

 - What is reinforcement learning?

 - Policy learning in reinforcement learning

What is multi-head latent attention (MLA)?

  Generative LLMs output one word a time. Each round, it uses previous words to predict the next word. So, people cache K and V values of previous words to avoid duplicated computation - called KV cache.

  A KV cache is two big matrixes (embedding dimension * dimension per head), and multi-head attention makes it even bigger (* number of heads). This requires lots of GPU memory during inference (model prediction).

  To reduce GPU memory, DeepSeek creates a latent vector for KV cache (lower dimension like PCA), by jointly compressing the keys and values.

 - What is multi-head attention?

 - What is an attention head?

What is a feed-forward network of transformers?

  A feed-forward network is a network in which information only moves in one direction: it has no cycles.

 - Neural networks
 - What is a transformer?

What is mixture of experts (MOE)?

  A mixture of experts machine learning model is one model that asks several other models (called "experts") to make predictions, and then uses those predictions to make its own prediction.

  Some common ways to combine predictions from individual experts:

  * Have each expert compute its own probability, give different weights to different experts, and compute a weighted sum (dense MOE)
  * Rank experts by their confidence, pick the top K experts, and compute a weighted sum (sparse MOE).

  MOE can help scale-up transformers, reducing computation costs.

 - What is a feed-forward network of transformers?

What is DeepSeekMOE?

  1) fine grained expert segmentation: use a larger number of smaller experts (double the number, half the parameters compared to other conventional MoE models). Each expert has less variants, together they make better decisions.

  2) shared experts: to store common knolwedge, which is always picked during inference.

  Problem: load balancing problem: during training, the model might reinforce the most strongest experts - might not be able to generalize.

  Solution:

  1) give loss or penalty to experts that received many training tokens.

  2) loss-free: add a bias in softmax, bias is controlled by how many training tokens an expert received (more tokens, less bias). This bias does not impact the loss function and inference.

 - What is mixture of experts (MOE)?

What else does DeepSeekMOE do?

  1) Node-limited routing: Moving tokens from a GPU node to another is expensive. DeepSeek sets a limit that each token can only be transferred to `m` GPU nodes.

  2) Token dropping: when some experts have high load balance, tokens would skip the feed-forward network and go directly to residue layer (directly add the input to the output layer).

 - What is DeepSeekMOE?

How to train LLMs?

  The purpose of an LLM is to output text. Training an LLM involves asking it to generate text, assigning a score to the text that it generated, and changing its parameters in a way that improves the score.

 - Large Language Models

 - Training neural networks

What is teacher forcing?

  Teacher forcing is the idea of asking an LLM to generate one token and scoring that one token alone, rather than asking the model to generate an entire sentence at once.

  After the model generates one token, regardless of whether that token was correct or not, the next step is to give the model the correct token and ask it to generate the following token.

  When an LLM generates one token at a time, this should be:

  * faster: it's not necessary to generate the entire sentence
  * more meaningful. If the model predicts that token #2 is the best to follow token #1, but it turns out that token #1 isn't the best starting token, it doesn't necessarily mean that token #2 isn't the best token to follow token #1.

 - How to train LLMs?

What is multi-token prediction (MTP)?

  [Multi-token prediction](https://www.clioapp.ai/research/multi-token-prediction) in an LLM involves splitting one head into several heads: head 1 predicts the next token, head 2 predicts the next next, heads 3 predicts the next next next, etc. Each head makes its predictions independently of the other heads.

  During training, the overall loss for the model is based on the combined losses of each head.

  During inference, there are two possible approaches:

  * Only the first head is used, and the other heads are ignored. This is expected to be more accurate.

  * Each head is used to generate its own token in parallel. This is expected to be faster.

  Advantages:

  * Speed: predicting the next `N` tokens generally requires less than `N` times as much time but allows us to compute `N` different scores (one per head per token), so this takes less time per score
  * Accuracy:
      * Sometimes a word in a sentence isn't very important and can be replaced with another word.
      * Sometimes a word in a sentence is important, and to change it requires changing other words in the sentence.
      * If you train an LLM by giving it the beginning of the desired text and asking it to predict the next token in the sentence, then getting either of these cases wrong produces an equal loss (1 token).
      * However, the case of dependent words is more important than the case of mostly interchangeable words, and deserves to be assigned a larger loss.
          * This larger loss is produced with multi-token prediction for other heads that predict subsequent words.
      * This is expected to cause the model to prioritize these important words more highly.

  Disadvantage:

  Not autoregressive or causal, because head 2 and 3 do not rely on head 1.

 - How to train LLMs?
 - What is an attention head?
 - What is teacher forcing?

Why LLM inference is expensive/slow?

  Transferring data and parameters (huge) from GPU memory to cache (for matrix dot computation) is slow.

  Can we use small LLMs to speed up big LLMs?

  e.g. Small models compute all answers. Big LLMs can check the answers of small LLMs and recompute those that are wrong.

 - What is a transformer?

Speculative decode

  1) quick guess: small LLMs -> even better some heads with small parameters in big LLMs, e.g. medusa or EAGLE

  2) cheap verification: big LLMs, check whether the probability of each token is the biggest.

  3) recompute: accept correct tokens and recompute the wrong token -> small LLMs recompute the next token

 - Why LLM inference is expensive/slow?

What is DeepSeekMTP?

  Main model: traditional transformer model, predict the next tokens

  MTP module 1: predict the next next tokens

  MTP module 2: predict the next next next tokens

  Main model gives its feature to the MTP module 1, concat with the MTP module 1 input embedding -> causal/autoregressive

  MTP module 1 gives its feature to the MTP module 2 -> causal/autoregressive

 - Speculative decode

 - What is multi-token prediction (MTP)?

DeepSeek-R1

  It's a reasoning model based on DeepSeek V3.

  DeepSeek-R1-Zero only trained with reinforcement learning without supervised fine-tuning (SFT) has poor readability, and language mixing.

  DeepSeek-R1 uses a small set of starting data (cold-start) for fine-tuning, and then do reinforcement learning with bootstrapping (generating new SFT synthetic data to train in iterations).

  I think 1) the cold-start starting data is really important.

  2) DeepSeek sets many different rules/policies to decide scores for an answer, instead of using human beings.

  Rules/policies for math and coding such as accuracy, language consistency, compiler should not fail...

  I think this is the future of AI -> sets good rules/policies in our domain to help AI learn based on our knowledge!

  3) DeepSeek-R1 generates new training data, and filtered by human beings.

  4) Second round of RL of DeepSeek-R1 on the new training data

  SFT -> RL -> SFT -> RL is a very efficient way of machine learning.

  Another improvement of DeepSeek-R1 compared to GPT-o is that it removes the reward model (a small model trained based on human feedback) from the reinforcement learning, but uses rules to give feedback.

  I think another cool part of DeepSeek-R1 reinforcement learning is that it regulates loss optimization by small steps (by clipping) to avoid jumping too far.

  This made their training very stable and efficient.

 - What is reinforcement learning?

Knowledge distillation

  Use a big LLM (teacher) to train a small LLM (student). Usually generates better performance than directly trainin a small LLM.

 - What is a transformer?

Machine learning scaling law

  Machine learning model performance usually shows a linear regression with computational time (log-scale), data size (log-scale), a total number of parameters (log-scale).
